ARG CUSTOM_LLVM=false

FROM registry.access.redhat.com/ubi9/ubi:latest AS llvm-build
ARG CUSTOM_LLVM
USER 0
# Conditionally execute the build based on CUSTOM_LLVM
RUN if [ "$CUSTOM_LLVM" = "true" ]; then \
        dnf update -y && \
        dnf -y install clang rpm-build git ninja-build cmake lld && \
        git clone https://github.com/llvm/llvm-project && \
        cd llvm-project && \
        COMMIT=$(curl -s https://raw.githubusercontent.com/triton-lang/triton/refs/heads/main/cmake/llvm-hash.txt) &&\
        git checkout $COMMIT && \
        mkdir build && \
        cd build && \
        cmake -G Ninja -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON ../llvm -DLLVM_ENABLE_PROJECTS="mlir;llvm" -DLLVM_TARGETS_TO_BUILD="host;NVPTX;AMDGPU" && \
        ninja; \
    else \
        echo "Skipping LLVM build because CUSTOM_LLVM is not true"; \
    fi

FROM registry.access.redhat.com/ubi9/python-312 AS base
ARG USERNAME=1001
ARG USER_UID=1000
ARG USER_GID=$USER_UID
ARG CUSTOM_LLVM
ARG INSTALL_CUDNN

USER 0
COPY user.sh user.sh

# Create the user
RUN ./user.sh -u $USERNAME -g $USER_GID
RUN dnf update -y && \
    dnf -y install clang cmake lld ninja-build;
# Set the user
USER $USERNAME

# Stage for llvm-local-true
FROM base AS llvm-local-true
COPY --from=llvm-build /llvm-project/ /llvm-project/

# Stage for llvm-local-false
FROM base AS llvm-local-false
ENV TRITON_OFFLINE_BUILD=NO

# Use intermediate stage selection
FROM llvm-local-${CUSTOM_LLVM}

ENV PYTHON_VERSION=3.12 \
    PATH=$HOME/.local/bin/:$PATH \
    PYTHONUNBUFFERED=1

FROM base AS triton-build
RUN git clone https://github.com/triton-lang/triton.git /triton
WORKDIR /triton
RUN git submodule init
RUN git submodule update
# Install dependencies
RUN pip install --upgrade pip
RUN pip install --upgrade setuptools
RUN pip install ninja cmake wheel pybind11
RUN if [[ "$INSTALL_CUDNN" == "true" ]]; then \
	python3 -m pip install nvidia-cudnn-cu12; \
fi

# Conditionally set the env vars based on CUSTOM_LLVM
RUN if [ "$CUSTOM_LLVM" = "true" ]; then \
    echo "export LLVM_BUILD_DIR=/llvm-project/build " >> "${HOME}/.bashrc" && \
    echo "export LLVM_INCLUDE_DIRS=/llvm-project/build /include" >> "${HOME}/.bashrc" && \
    echo "export LLVM_LIBRARY_DIR=/llvm-project/build /lib" >> "${HOME}/.bashrc" && \
    echo "export LLVM_SYSPATH=/llvm-project/build " >> "${HOME}/.bashrc"; \
fi

RUN pip wheel --wheel-dir=/wheelhouse -e python

FROM base AS pytorch-build
RUN git clone --recursive https://github.com/pytorch/pytorch.git /pytorch
WORKDIR /pytorch
RUN pip install -r requirements.txt
RUN pip install ninja cmake rust mkl-static mkl-include
COPY --from=triton-build /wheelhouse /wheelhouse
RUN pip install /wheelhouse/*.whl
ENV USE_CUDA=1 \
    BUILD_TEST=0 \
    TRITON_PATH="/opt/app-root/lib64/python3.12/site-packages/triton"
RUN python setup.py clean
RUN python setup.py bdist_wheel

FROM base AS triton-dev
RUN git clone https://github.com/triton-lang/triton.git /triton
WORKDIR /triton
RUN dnf install -y openblas openblas-devel llvm llvm-libs libomp libomp-devel && \
    dnf clean all
RUN pip install --upgrade pip
RUN pip install pre-commit
RUN pip install numpy matplotlib pandas tabulate scipy
COPY --from=triton-build /wheelhouse /wheelhouse
RUN pip install --no-cache-dir /wheelhouse/*.whl
COPY --from=pytorch-build /pytorch/dist /pytorch/dist
RUN pip install --no-cache-dir /pytorch/dist/*.whl
RUN echo 'export LD_LIBRARY_PATH=/usr/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
RUN echo "export MAX_JOBS=$(nproc --all)" >> "${HOME}"/.bashrc
COPY entrypoint.sh /entrypoint.sh
ENTRYPOINT ["/entrypoint.sh"]
CMD ["tail", "-f", "/dev/null"]
